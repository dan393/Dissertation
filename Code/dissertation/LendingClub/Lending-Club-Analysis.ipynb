{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import kr_helper_funcs as kr\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import kr_helper_funcs as kr\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "name = 'with_postcode'\n",
    "fileName = \"{}/lending-club-values.csv\".format(name)\n",
    "model = tf.keras.models.load_model('{}/lending-club.h5'.format(name))\n",
    "X_train_unscaled = np.load(\"{}/X_train_unscaled.npy\".format(name))\n",
    "X_test_unscaled = np.load(\"{}/X_test_unscaled.npy\".format(name))\n",
    "y_train = np.load(\"{}/y_test.npy\".format(name))\n",
    "y_test = np.load(\"{}/y_test.npy\".format(name))\n",
    "df = pd.read_csv('{}/lending-club-df.csv'.format(name))\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_unscaled)\n",
    "X_test = scaler.transform(X_test_unscaled)\n",
    "\n",
    "prev_scaled_row = None\n",
    "cached_map_values = None\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_unscaled)\n",
    "X_test = scaler.transform(X_test_unscaled)\n",
    "\n",
    "res = pd.read_csv(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['loan_amnt', 'term', 'int_rate', 'installment', 'annual_inc', 'dti',\n",
       "       'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc',\n",
       "       'mort_acc', 'pub_rec_bankruptcies', 'A2', 'A3', 'A4', 'A5', 'B1', 'B2',\n",
       "       'B3', 'B4', 'B5', 'C1', 'C2', 'C3', 'C4', 'C5', 'D1', 'D2', 'D3', 'D4',\n",
       "       'D5', 'E1', 'E2', 'E3', 'E4', 'E5', 'F1', 'F2', 'F3', 'F4', 'F5', 'G1',\n",
       "       'G2', 'G3', 'G4', 'G5', 'verification_status_Source Verified',\n",
       "       'verification_status_Verified', 'application_type_INDIVIDUAL',\n",
       "       'application_type_JOINT', 'initial_list_status_w',\n",
       "       'purpose_credit_card', 'purpose_debt_consolidation',\n",
       "       'purpose_educational', 'purpose_home_improvement', 'purpose_house',\n",
       "       'purpose_major_purchase', 'purpose_medical', 'purpose_moving',\n",
       "       'purpose_other', 'purpose_renewable_energy', 'purpose_small_business',\n",
       "       'purpose_vacation', 'purpose_wedding', 'OTHER', 'OWN', 'RENT', '05113',\n",
       "       '11650', '22690', '29597', '30723', '48052', '70466', '86630', '93700',\n",
       "       'earliest_cr_year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('loan_repaid', axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "If no scoring is specified, the estimator passed should have a 'score' method. The estimator <tensorflow.python.keras.engine.sequential.Sequential object at 0x00000252E0BDE588> does not.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b99a5e6a68f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# train_x, val_x, train_y, val_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mperm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPermutationImportance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0meli5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loan_repaid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\eli5\\sklearn\\permutation_importance.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \"\"\"\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscorer_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpandas_available\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[1;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[0;32m    293\u001b[0m                 \u001b[1;34m\"If no scoring is specified, the estimator passed should \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[1;34m\"have a 'score' method. The estimator %r does not.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 % estimator)\n\u001b[0m\u001b[0;32m    296\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         raise ValueError(\"For evaluating multiple scores, use \"\n",
      "\u001b[1;31mTypeError\u001b[0m: If no scoring is specified, the estimator passed should have a 'score' method. The estimator <tensorflow.python.keras.engine.sequential.Sequential object at 0x00000252E0BDE588> does not."
     ]
    }
   ],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "# X_train_unscaled, X_test_unscaled, y_train, y_test\n",
    "# train_x, val_x, train_y, val_y\n",
    "\n",
    "perm = PermutationImportance(model).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names = df.drop('loan_repaid', axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.explain_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package eli5:\n",
      "\n",
      "NAME\n",
      "    eli5 - # -*- coding: utf-8 -*-\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _decision_path\n",
      "    _feature_importances\n",
      "    _feature_names\n",
      "    _feature_weights\n",
      "    _graphviz\n",
      "    base\n",
      "    base_utils\n",
      "    catboost\n",
      "    explain\n",
      "    formatters (package)\n",
      "    ipython\n",
      "    keras (package)\n",
      "    lightgbm\n",
      "    lightning\n",
      "    lime (package)\n",
      "    permutation_importance\n",
      "    sklearn (package)\n",
      "    sklearn_crfsuite (package)\n",
      "    transform\n",
      "    utils\n",
      "    xgboost\n",
      "\n",
      "DATA\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "\n",
      "VERSION\n",
      "    0.10.1\n",
      "\n",
      "FILE\n",
      "    c:\\users\\dan39\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\eli5\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from eli5.lime import TextExplainer\n",
    "help(eli5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_prediction(model, X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.read_csv('{}/lending-club-history.csv'.format(name))\n",
    "history_dict = history.to_dict('list')\n",
    "kr.show_plots(history_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kr.plot_cm(y_test, predictions, [\"unpaid\", \"paid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"explainer\",data=res[(res['nsamples'] == 'auto') ], hue = 'class_change')\n",
    "plt.title(\"Number of rows predicted as the same class vs predicted as opposite class after columns have been neutralized based on the explainer's identified features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"no_features\",data=res[(res['nsamples'] == 'auto') ], hue = 'class_change')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_random = res[(res['nsamples'] == 'auto') & (res['explainer'] == 'random')].groupby(\"no_features\")[\"confidence_diff\"].mean() \n",
    "avg_shap = res[(res['nsamples'] == 'auto') & (res['explainer'] == 'shap')].groupby(\"no_features\")[\"confidence_diff\"].mean()\n",
    "avg_lime = res[(res['nsamples'] == 'auto') & (res['explainer'] == 'lime')].groupby(\"no_features\")[\"confidence_diff\"].mean()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "# plt.set_title('Training & Validation Loss')\n",
    "no_of_features = list(range(0, len(avg_random) ))\n",
    "plt.plot( no_of_features, avg_random, color='navy', marker='*', linestyle='-', label='Random')\n",
    "plt.plot( no_of_features, avg_shap, color='green', marker='x', linestyle='-', label='Shap')\n",
    "plt.plot( no_of_features, avg_lime, color='red', marker='o', linestyle='-', label='Lime')\n",
    "plt.title(\"Average of confidence level change vs no of features neutralized. If no feature is altered the (old prediction - new prediction) == 0 should stay the same. The more features we change, the higher should the difference increase\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as the graph above but the plot is class specific, in this case for class 1. For class 0, there are not enough datapoints to plot the graph\n",
    "original_class = 1\n",
    "avg_random = res[(res['nsamples'] == 'auto') & (res['explainer'] == 'random') & (res['original_class'] == original_class)].groupby(\"no_features\")[\"confidence_diff\"].mean() \n",
    "avg_shap = res[(res['nsamples'] == 'auto') & (res['explainer'] == 'shap') & (res['original_class'] == original_class)].groupby(\"no_features\")[\"confidence_diff\"].mean()\n",
    "avg_lime = res[(res['nsamples'] == 'auto') & (res['explainer'] == 'lime') & (res['original_class'] == original_class)].groupby(\"no_features\")[\"confidence_diff\"].mean()\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "# plt.set_title('Training & Validation Loss')\n",
    "no_of_features = list(range(0, len(avg_random) ))\n",
    "plt.plot( no_of_features, avg_random, color='navy', marker='*', linestyle='-', label='Random')\n",
    "plt.plot( no_of_features, avg_shap, color='green', marker='x', linestyle='-', label='Shap')\n",
    "plt.plot( no_of_features, avg_lime, color='red', marker='o', linestyle='-', label='Lime')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[res[\"original_class\"] == 0].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# avg = res[res['confidence_diff'] == 0.0].groupby(\"no_features\")[\"confidence_diff\"].count() / res.groupby(\"no_features\")[\"confidence_diff\"].count() # / res[(res['nsamples'] == 'auto') & (res['explainer'] == 'random')].groupby(\"no_features\")[\"confidence_diff\"].count()\n",
    "avg_random = res[(res['confidence_diff'] < 0.01) & (res['nsamples'] == 'auto') & (res['explainer'] == 'random')].groupby(\"no_features\")[\"confidence_diff\"].count() / res[(res['nsamples'] == 'auto') & (res['explainer'] == 'random')].groupby(\"no_features\")[\"confidence_diff\"].count()\n",
    "avg_shap = res[(res['confidence_diff'] < 0.01) & (res['nsamples'] == 'auto') & (res['explainer'] == 'shap')].groupby(\"no_features\")[\"confidence_diff\"].count() / res[(res['nsamples'] == 'auto') & (res['explainer'] == 'shap')].groupby(\"no_features\")[\"confidence_diff\"].count()\n",
    "avg_lime = res[(res['confidence_diff'] < 0.01) & (res['nsamples'] == 'auto') & (res['explainer'] == 'lime')].groupby(\"no_features\")[\"confidence_diff\"].count() / res[(res['nsamples'] == 'auto') & (res['explainer'] == 'lime')].groupby(\"no_features\")[\"confidence_diff\"].count()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "# plt.set_title('Training & Validation Loss')\n",
    "no_of_features = list(range(0, len(avg_shap) ))\n",
    "plt.plot( no_of_features, avg_random, color='navy', marker='*', linestyle='-', label='Random Confidence level change vs no of features')\n",
    "plt.plot( no_of_features, avg_shap, color='green', marker='x', linestyle='-', label='Shap Confidence level change vs no of features')\n",
    "plt.plot( no_of_features, avg_lime, color='red', marker='o', linestyle='-', label='Lime Confidence level change vs no of features')\n",
    "plt.title(\"Percentage of rows where the new prediction favours the original class instead of decreasing its probability or has no impact. (old prediction - new prediction) <=0.1\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "avg_random = res[(res[\"no_features\"]==0) & (res['explainer'] == 'random')].groupby(\"nsamples\")[\"time\"].mean() \n",
    "avg_shap = res[(res[\"no_features\"]==0) & (res['explainer'] == 'shap')].groupby(\"nsamples\")[\"time\"].mean()\n",
    "avg_lime = res[(res[\"no_features\"]==0) & (res['explainer'] == 'lime')].groupby(\"nsamples\")[\"time\"].mean()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "# plt.set_title('Training & Validation Loss')\n",
    "no_of_features = list(range(0, len(avg_random) ))\n",
    "plt.plot( no_of_features, avg_random, color='navy', marker='*', linestyle='-', label='Random')\n",
    "plt.plot( no_of_features, avg_shap, color='green', marker='x', linestyle='-', label='Shap')\n",
    "plt.plot( no_of_features, avg_lime, color='red', marker='o', linestyle='-', label='Lime')\n",
    "plt.title(\"Average time taken for each explainer to compute top 30 most important features of a row based on the nsamples allowed to compute\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_shap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
