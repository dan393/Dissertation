{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from pandas.plotting import table \n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensorflow2.0.0\n",
      "tensorboard2.0.2\n",
      "seaborn0.9.0\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ac37a3985a88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mseaborn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'seaborn'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mseaborn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_built_with_cuda\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Num GPUs Available: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core._api.v2.config' has no attribute 'list_physical_devices'"
     ],
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core._api.v2.config' has no attribute 'list_physical_devices'",
     "output_type": "error"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('tensorflow' + tf.__version__)\n",
    "import tensorboard as tensorboard\n",
    "print('tensorboard' + tensorboard.__version__)\n",
    "import seaborn as seaborn\n",
    "print('seaborn' + seaborn.__version__)\n",
    "tf.config.list_physical_devices('GPU') \n",
    "tf.test.is_built_with_cuda\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def save(path):\n",
    "    plt.savefig('../../images' + path)\n",
    "    return\n",
    "\n",
    "def loadDataSet():\n",
    "#     df  = pd.read_csv(\"../input/stroke-dataset/stroke_subset.csv\")\n",
    "    return pd.read_csv(\"../input/stroke-dataset/stroke.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df=loadDataSet();\n",
    "df = df.drop('id',axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.countplot(x='stroke',data=df)\n",
    "save('/building_ann/stroke_countplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.distplot(df.loc[df['stroke'] == 1]['age'], label='Stroke Patients', kde_kws={\"shade\": True},hist=False, bins =20)\n",
    "sns.distplot(df.loc[df['heart_disease'] == 1]['age'], label='Heart Disease Patients', kde_kws={\"shade\": True},hist=False, bins =20)\n",
    "sns.distplot(df.loc[df['hypertension'] == 1]['age'], label='Hypertension', kde_kws={\"shade\": True},hist=False, bins =20)\n",
    "plt.legend()\n",
    "save('/building_ann/stroke_age_distplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[95]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "df_hm = loadDataSet()\n",
    "le = preprocessing.LabelEncoder()\n",
    "df_hm.gender = le.fit_transform(df_hm.gender)\n",
    "df_hm.ever_married = le.fit_transform(df_hm.ever_married)\n",
    "df_hm=df_hm.dropna(subset=['smoking_status'])\n",
    "df_hm.smoking_status = le.fit_transform(df_hm.smoking_status)\n",
    "df_hm.Residence_type = le.fit_transform(df_hm.Residence_type)\n",
    "df_hm.work_type = le.fit_transform(df_hm.work_type)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(df_hm.corr(),annot=True,cmap='viridis', linewidth=1)\n",
    "plt.ylim(12, 0)\n",
    "save('/building_ann/heatmap.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(25,25))\n",
    "# sns.pairplot(df_hm)\n",
    "# save('/building_ann/pairplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.scatterplot(x='age',y='bmi', hue='stroke', data=df)\n",
    "save('/building_ann/age_bmi_scatterplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "df = loadDataSet()\n",
    "sns.countplot(x='smoking_status',data=df,hue='gender')\n",
    "save('/building_ann/smoking_scatterplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(df[df['stroke'] == 1 ])/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Adapted code from: https://github.com/mwaskom/seaborn/issues/1027\n",
    "plt.figure(figsize=(5,5))\n",
    "df = loadDataSet()\n",
    "genderOther = df[ df['gender'] == 'Other' ].index\n",
    "df.drop(genderOther , inplace=True)\n",
    "x, y, hue = \"smoking_status\", \"percentage\", \"gender\"\n",
    "# hue_order = [\"Male\", \"Female\"]\n",
    "\n",
    "#f, axes = plt.subplots(1, 2)\n",
    "# sns.countplot(x=x, hue=hue, data=df, ax=axes[0])\n",
    "prop_df = (df[x]\n",
    "           .groupby(df[hue])\n",
    "           .value_counts(normalize=True)\n",
    "           .rename(y)\n",
    "           .reset_index())\n",
    "\n",
    "sns.barplot(x=x, y=y, hue=hue, data=prop_df)#, ax=axes[1])\n",
    "save('/building_ann/gender_smoking_percentage.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "stroke_y = df[df['stroke'] == 1 ].groupby(\"smoking_status\").count()['stroke']\n",
    "stroke_n = df[df['stroke'] == 0 ].groupby(\"smoking_status\").count()['stroke']\n",
    "print(stroke_y/(stroke_y + stroke_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(\"smoking_status\")['stroke'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "np.bincount(df.apply(lambda x : 1 if x['gender']=='Male' else 0, axis =1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "np.bincount(df['stroke'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def fill_smoking_status(smoking_status, work_type, gender, age):\n",
    "    if not pd.isnull(smoking_status):\n",
    "        return smoking_status\n",
    "    if work_type=='children' or gender =='Female' or age<18:\n",
    "        return 'never smoked'\n",
    "    return 'smokes'\n",
    "df = loadDataSet()\n",
    "df['smoking_status'] = df.apply(lambda x : fill_smoking_status(x['smoking_status'], x['work_type'], x['gender'], x['age']), axis=1)\n",
    "smoking_dummies = pd.get_dummies(df['smoking_status'],drop_first=True)\n",
    "df = pd.concat([df.drop('smoking_status',axis=1),smoking_dummies],axis=1)\n",
    "\n",
    "df['gender'] = df['gender'].replace([\"Other\"], \"Male\")\n",
    "gender_dummies = pd.get_dummies(df['gender'],drop_first=True)\n",
    "df = pd.concat([df.drop('gender',axis=1),gender_dummies],axis=1)\n",
    "\n",
    "bmi_avg= df.groupby('work_type').mean()['bmi']\n",
    "df['bmi'] = df.apply(lambda x : bmi_avg[x['work_type']] if np.isnan(x['bmi']) else x['bmi'], axis =1)\n",
    "\n",
    "\n",
    "work_type_dummies = pd.get_dummies(df['work_type'],drop_first=True)\n",
    "df = pd.concat([df.drop('work_type',axis=1),work_type_dummies],axis=1)\n",
    "\n",
    "married_dummies = pd.get_dummies(df['ever_married'],drop_first=True)\n",
    "df = pd.concat([df.drop('ever_married',axis=1),married_dummies],axis=1)\n",
    "df = df.rename(columns={'Yes': 'Married'})\n",
    "\n",
    "residence_dummies = pd.get_dummies(df['Residence_type'],drop_first=True)\n",
    "df = pd.concat([df.drop('Residence_type',axis=1),residence_dummies],axis=1)\n",
    "\n",
    "df = df.drop('id',axis=1)\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# def fill_smoking_status(smoking_status, work_type, gender, age):\n",
    "#     if not pd.isnull(smoking_status):\n",
    "#         return smoking_status\n",
    "#     if work_type=='children' or gender =='Female' or age<18:\n",
    "#         return 'never smoked'\n",
    "#     return 'smokes'\n",
    "\n",
    "# df = loadDataSet()\n",
    "# df = df.drop('smoking_status',axis=1)\n",
    "# df = df.dropna(subset=['bmi'])\n",
    "# df = df.drop('bmi',axis=1)\n",
    "# df = df.drop('gender',axis=1)\n",
    "# df = df.drop('work_type',axis=1)\n",
    "# df = df.drop('ever_married',axis=1)\n",
    "# df = df.drop('Residence_type',axis=1)\n",
    "# # df['smoking_status'] = df.apply(lambda x : fill_smoking_status(x['smoking_status'], x['work_type'], x['gender'], x['age']), axis=1)\n",
    "# # smoking_dummies = pd.get_dummies(df['smoking_status'],drop_first=True)\n",
    "# # df = pd.concat([df.drop('smoking_status',axis=1),smoking_dummies],axis=1)\n",
    "\n",
    "# # df['gender'] = df['gender'].replace([\"Other\"], \"Male\")\n",
    "# # gender_dummies = pd.get_dummies(df['gender'],drop_first=True)\n",
    "# # df = pd.concat([df.drop('gender',axis=1),gender_dummies],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# df = df.drop('id',axis=1)\n",
    "\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# # sorted(df['smoking_status'].unique())\n",
    "# plt.figure(figsize=(10,10))\n",
    "# sns.countplot(x='stroke',data=df,hue='smoking_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop('stroke', axis =1).values\n",
    "y = df['stroke'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "help(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_roc(name, labels, predictions, ax, **kwargs):\n",
    "    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "    lw = 2\n",
    "    plt.plot(fp, tp, color='darkorange',\n",
    "             lw=lw, label='ROC curve '+ name, **kwargs)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "def save_report_to_latex(model_name, report):\n",
    "    df_latex = pd.DataFrame(report).transpose()\n",
    "#     ax = plt.subplot(441, frame_on=False)\n",
    "#     ax.xaxis.set_visible(False)  # hide the x axis\n",
    "#     ax.yaxis.set_visible(False)  \n",
    "#     table(ax, data=df_latex)\n",
    "#     plt.savefig('../../images/building_ann/cr_' + model_name + '.png')\n",
    "    \n",
    "    with open('../../images/building_ann/' + model_name + '.tex','w') as tf:\n",
    "        tf.write(df_latex.to_latex())\n",
    "    return\n",
    "\n",
    "# part of this code is inspired from https://www.tensorflow.org/tutorials/structured_data/imbalanced_data\n",
    "def plot_model_results(model_name, model):\n",
    "    fig, (f1, f2, f3) =plt.subplots(1,3, figsize=(17,6))\n",
    "    predictions = model.predict_classes(X_test)\n",
    "    report = classification_report(y_test,predictions)\n",
    "    print(report)\n",
    "    report_json = classification_report(y_test,predictions, output_dict=True)\n",
    "    save_report_to_latex(model_name, report_json)\n",
    "    \n",
    "    \n",
    "    losses = pd.DataFrame(model.history.history)\n",
    "    fig1 = losses[['loss','val_loss']].plot(ax=f1) \n",
    "    fig1.set_ylabel('Cost')\n",
    "    fig1.set_xlabel('Epochs')\n",
    "    \n",
    "    cm = confusion_matrix(y_test,predictions > 0.5)\n",
    "    fig2= sns.heatmap(cm, annot=True, fmt=\"d\", ax=f2)\n",
    "    fig2.set_title('Confusion matrix @{:.2f}'.format(0.5))\n",
    "    fig2.set_ylabel('Actual label')\n",
    "    fig2.set_xlabel('Predicted label')\n",
    "    fig2.set_ylim(2, 0)\n",
    "\n",
    "    fig3 = plot_roc(\"Train Baseline\",y_train, model.predict_classes(X_train) , ax=f3)\n",
    "    fig3 = plot_roc(\"Test Baseline\", y_test, model.predict_classes(X_test), ax=f3, linestyle='--')\n",
    "    save('/building_ann/' + model_name + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "help(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from datetime import datetime\n",
    "from tensorflow import keras\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "        f1_m\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def create_model(model_name, X_train=X_train, y_train=y_train, with_weigths=True):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30,  activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(15,  activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',  metrics=METRICS )\n",
    "    \n",
    "    class_weight= {0:1, 1:1}\n",
    "    if (with_weigths):\n",
    "        neg, pos = np.bincount(y_train)\n",
    "        total = neg + pos\n",
    "        print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "        total, pos, 100 * pos / total))\n",
    "        weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "        weight_for_1 = (1 / pos)*(total)/2.0\n",
    "        class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "        print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "        print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "        \n",
    "    model.fit(x=X_train, \n",
    "              y=y_train, \n",
    "              epochs=500,\n",
    "              class_weight=class_weight,\n",
    "              batch_size=250,\n",
    "              validation_data=(X_test, y_test), \n",
    "              verbose=0,\n",
    "              callbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)]\n",
    "              )\n",
    "    \n",
    "    plot_model_results(model_name,model)\n",
    "    return model\n",
    "    \n",
    "#     return model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def create_model_logistic(model_name, X_train=X_train, y_train=y_train, with_weigths=True):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    \n",
    "    class_weight= {0:1, 1:2}\n",
    "    if (with_weigths):\n",
    "        neg, pos = np.bincount(y_train)\n",
    "        total = neg + pos\n",
    "        print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "        total, pos, 100 * pos / total))\n",
    "        weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "        weight_for_1 = (1 / pos)*(total)/2.0\n",
    "        class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "        print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "        print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "\n",
    "    logit = LogisticRegression(C=1, class_weight= class_weight, random_state = 123, solver='saga')\n",
    "    \n",
    "    logit.fit(X_train, y_train)\n",
    "    predictions = logit.predict(X_test)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    \n",
    "    \n",
    "#     return model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "create_model('weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# results[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "create_model('simple', with_weigths=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(np.bincount(y_train))\n",
    "smt = SMOTE()\n",
    "X_train_SMOTE, y_train_SMOTE = smt.fit_sample(X_train, y_train)\n",
    "print(np.bincount(y_train_SMOTE))\n",
    "create_model('smote',X_train_SMOTE,y_train_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(np.bincount(y_train))\n",
    "nr = NearMiss()\n",
    "X_train_miss, y_train_miss = nr.fit_sample(X_train, y_train)\n",
    "print(np.bincount(y_train_miss))\n",
    "create_model('near_miss',X_train_miss,y_train_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# print(np.bincount(y_train))\n",
    "# cluster = ClusterCentroids()\n",
    "# X_train_cluster, y_train_cluster = cluster.fit_sample(X_train, y_train)\n",
    "# print(np.bincount(y_train_cluster))\n",
    "# create_model('cluster', X_train_cluster, y_train_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(np.bincount(y_train))\n",
    "smt = SMOTETomek('auto')\n",
    "X_train_SMTomek, y_train_SMTomek = smt.fit_sample(X_train, y_train)\n",
    "print(np.bincount(y_train_SMTomek))\n",
    "create_model('SMOTETomek', X_train_SMTomek, y_train_SMTomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(np.bincount(y_train))\n",
    "rus = RandomUnderSampler()\n",
    "X_train_rus, y_train_rus = rus.fit_sample(X_train, y_train)\n",
    "print(np.bincount(y_train_rus))\n",
    "model = create_model('rus', X_train_rus, y_train_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(np.bincount(y_train))\n",
    "ros = RandomOverSampler()\n",
    "X_train_ros, y_train_ros = ros.fit_sample(X_train, y_train)\n",
    "print(np.bincount(y_train_ros))\n",
    "create_model('ros', X_train_ros, y_train_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(np.bincount(y_train))\n",
    "smte = SMOTEENN('auto')\n",
    "X_train_smte, y_train_smte = smt.fit_sample(X_train, y_train)\n",
    "print(np.bincount(y_train_smte))\n",
    "create_model('smte', X_train_smte, y_train_smte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp\n",
    "import os\n",
    "HP_NUM_UNITS_L1 = hp.HParam('num_units_l1', hp.Discrete([32, 49, 79]))\n",
    "HP_NUM_UNITS_L2 = hp.HParam('num_units_l2', hp.Discrete([32, 16,8]))\n",
    "HP_NUM_UNITS_L3 = hp.HParam('num_units_l3', hp.Discrete([8,4]))\n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([250,1000]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.4, 0.5))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "HP_LOSS = hp.HParam('loss', hp.Discrete(['mse', 'binary_crossentropy']))\n",
    "\n",
    "# HP_NUM_UNITS_L1 = hp.HParam('num_units_l1', hp.Discrete([16]))\n",
    "# HP_NUM_UNITS_L2 = hp.HParam('num_units_l2', hp.Discrete([4]))\n",
    "# HP_NUM_UNITS_L3 = hp.HParam('num_units_l3', hp.Discrete([3]))\n",
    "# HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.3))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "# HP_LOSS = hp.HParam('loss', hp.Discrete(['binary_crossentropy']))\n",
    "\n",
    "# print('Deleting previous logs')\n",
    "# !del -f \"logs\\\\\"\n",
    "# print('Delet previous logs')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# base_dir = 'logs\\\\hparam_tuning\\\\' + datetime.now().strftime(\"%Y-%m-%d-%H%M\") + '\\\\'\n",
    "base_dir = os.path.join('logs', 'hparam_tuning', datetime.now().strftime(\"%Y-%m-%d-%H%M\"))\n",
    "if not os.path.exists(base_dir):\n",
    "        os.mkdir(base_dir)\n",
    "        \n",
    "\n",
    "def train_test_model(model_name, hparams, logdir, X_train=X_train, y_train=y_train, with_weigths=True):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hparams[HP_NUM_UNITS_L1],  activation='relu'))\n",
    "    model.add(Dropout(hparams[HP_DROPOUT]))\n",
    "    model.add(Dense(hparams[HP_NUM_UNITS_L2],  activation='relu'))\n",
    "    model.add(Dropout(hparams[HP_DROPOUT]))\n",
    "    model.add(Dense(hparams[HP_NUM_UNITS_L3], activation='relu'))\n",
    "    model.add(Dropout(hparams[HP_DROPOUT]))\n",
    "    model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=hparams[HP_LOSS], optimizer=hparams[HP_OPTIMIZER],  metrics=METRICS )\n",
    "    \n",
    "    class_weight= {0:1, 1:1}\n",
    "    if (with_weigths):\n",
    "        neg, pos = np.bincount(y_train)\n",
    "        total = neg + pos\n",
    "        print('Examples: Total: {} Positive: {} ({:.2f}% of total)'.format(\n",
    "        total, pos, 100 * pos / total))\n",
    "        weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "        weight_for_1 = (1 / pos)*(total)/2.0\n",
    "        class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "        print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "        print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "    \n",
    "    \n",
    "    model.fit(x=X_train, \n",
    "              y=y_train, \n",
    "              epochs=500,\n",
    "              class_weight=class_weight,\n",
    "              batch_size=hparams[HP_BATCH_SIZE],\n",
    "              validation_data=(X_test, y_test), \n",
    "              verbose=0,\n",
    "              callbacks = [EarlyStopping(monitor='loss', mode='min', verbose=1, patience=30), \n",
    "                           tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1, write_graph=False,\n",
    "                                                          write_images=False, update_freq='epoch', \n",
    "                                                          profile_batch = 100000000),  # log metrics\n",
    "                           hp.KerasCallback(logdir, hparams)]\n",
    "              )\n",
    "    \n",
    "#     plot_model_results(model_name,model)\n",
    "    \n",
    "    \n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    print(results)\n",
    "    return results\n",
    "\n",
    "def run(run_name, hparams,  X_train=X_train, y_train=y_train):\n",
    "    log_dir= os.path.join(base_dir, run_name)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.mkdir(log_dir)\n",
    "    with tf.summary.create_file_writer(log_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        results = train_test_model(run_name, hparams, log_dir, X_train, y_train)\n",
    "        tf.summary.scalar('loss', results[0], step=1)\n",
    "        tf.summary.scalar('tp', results[1], step=1)\n",
    "        tf.summary.scalar('fp', results[2], step=1)\n",
    "        tf.summary.scalar('tn', results[3], step=1)\n",
    "        tf.summary.scalar('fn', results[4], step=1)\n",
    "        tf.summary.scalar('accuracy', results[5], step=1)\n",
    "        tf.summary.scalar('precision', results[6], step=1)\n",
    "        tf.summary.scalar('recall', results[7], step=1)\n",
    "        tf.summary.scalar('auc', results[8], step=1)\n",
    "        tf.summary.scalar('f1-score', results[9], step=1)\n",
    "                                     \n",
    "def hrun(dataset_name='default', X_train=X_train, y_train=y_train):\n",
    "    session_num=0\n",
    "    for num_units_l1 in HP_NUM_UNITS_L1.domain.values:\n",
    "        for num_units_l2 in HP_NUM_UNITS_L2.domain.values:\n",
    "            for num_units_l3 in HP_NUM_UNITS_L3.domain.values:\n",
    "                for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "                    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "                        for loss in HP_LOSS.domain.values:\n",
    "                            for batch_size in HP_BATCH_SIZE.domain.values:\n",
    "                                hparams = {\n",
    "                                  HP_NUM_UNITS_L1: num_units_l1,\n",
    "                                  HP_NUM_UNITS_L2: num_units_l2,\n",
    "                                  HP_NUM_UNITS_L3: num_units_l3,\n",
    "                                  HP_DROPOUT: dropout_rate,\n",
    "                                  HP_OPTIMIZER: optimizer,\n",
    "                                  HP_LOSS: loss,\n",
    "                                  HP_BATCH_SIZE: batch_size}\n",
    "                                run_name =dataset_name + \"-run-%d\" % session_num\n",
    "                                print('\\n--- ----------Starting trial:',  run_name, '--------------------------')\n",
    "                                print({h.name: hparams[h] for h in hparams})\n",
    "                                run(run_name, hparams,  X_train, y_train)\n",
    "                                session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(np.bincount(y_train))\n",
    "ros = RandomOverSampler()\n",
    "X_train_ros, y_train_ros = ros.fit_sample(X_train, y_train)\n",
    "print(np.bincount(y_train_ros))\n",
    "create_model('ros', X_train_ros, y_train_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(np.bincount(y_train))\n",
    "smte = SMOTEENN('auto')\n",
    "X_train_smte, y_train_smte = smt.fit_sample(X_train, y_train)\n",
    "print(np.bincount(y_train_smte))\n",
    "create_model('smte', X_train_smte, y_train_smte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as hp\n",
    "import os\n",
    "HP_NUM_UNITS_L1 = hp.HParam('num_units_l1', hp.Discrete([32, 49, 79]))\n",
    "HP_NUM_UNITS_L2 = hp.HParam('num_units_l2', hp.Discrete([32, 16,8]))\n",
    "HP_NUM_UNITS_L3 = hp.HParam('num_units_l3', hp.Discrete([8,4]))\n",
    "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([250,1000]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.4, 0.5))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "HP_LOSS = hp.HParam('loss', hp.Discrete(['mse', 'binary_crossentropy']))\n",
    "\n",
    "# HP_NUM_UNITS_L1 = hp.HParam('num_units_l1', hp.Discrete([16]))\n",
    "# HP_NUM_UNITS_L2 = hp.HParam('num_units_l2', hp.Discrete([4]))\n",
    "# HP_NUM_UNITS_L3 = hp.HParam('num_units_l3', hp.Discrete([3]))\n",
    "# HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.3))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "# HP_LOSS = hp.HParam('loss', hp.Discrete(['binary_crossentropy']))\n",
    "\n",
    "# print('Deleting previous logs')\n",
    "# !del -f \"logs\\\\\"\n",
    "# print('Delet previous logs')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# base_dir = 'logs\\\\hparam_tuning\\\\' + datetime.now().strftime(\"%Y-%m-%d-%H%M\") + '\\\\'\n",
    "base_dir = os.path.join('logs', 'hparam_tuning', datetime.now().strftime(\"%Y-%m-%d-%H%M\"))\n",
    "if not os.path.exists(base_dir):\n",
    "        os.mkdir(base_dir)\n",
    "        \n",
    "\n",
    "def train_test_model(model_name, hparams, logdir, X_train=X_train, y_train=y_train, with_weigths=True):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hparams[HP_NUM_UNITS_L1],  activation='relu'))\n",
    "    model.add(Dropout(hparams[HP_DROPOUT]))\n",
    "    model.add(Dense(hparams[HP_NUM_UNITS_L2],  activation='relu'))\n",
    "    model.add(Dropout(hparams[HP_DROPOUT]))\n",
    "    model.add(Dense(hparams[HP_NUM_UNITS_L3], activation='relu'))\n",
    "    model.add(Dropout(hparams[HP_DROPOUT]))\n",
    "    model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=hparams[HP_LOSS], optimizer=hparams[HP_OPTIMIZER],  metrics=METRICS )\n",
    "    \n",
    "    class_weight= {0:1, 1:1}\n",
    "    if (with_weigths):\n",
    "        neg, pos = np.bincount(y_train)\n",
    "        total = neg + pos\n",
    "        print('Examples: Total: {} Positive: {} ({:.2f}% of total)'.format(\n",
    "        total, pos, 100 * pos / total))\n",
    "        weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "        weight_for_1 = (1 / pos)*(total)/2.0\n",
    "        class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "        print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "        print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "    \n",
    "    \n",
    "    model.fit(x=X_train, \n",
    "              y=y_train, \n",
    "              epochs=500,\n",
    "              class_weight=class_weight,\n",
    "              batch_size=hparams[HP_BATCH_SIZE],\n",
    "              validation_data=(X_test, y_test), \n",
    "              verbose=0,\n",
    "              callbacks = [EarlyStopping(monitor='loss', mode='min', verbose=1, patience=30), \n",
    "                           tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1, write_graph=False,\n",
    "                                                          write_images=False, update_freq='epoch', \n",
    "                                                          profile_batch = 100000000),  # log metrics\n",
    "                           hp.KerasCallback(logdir, hparams)]\n",
    "              )\n",
    "    \n",
    "#     plot_model_results(model_name,model)\n",
    "    \n",
    "    \n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    print(results)\n",
    "    return results\n",
    "\n",
    "def run(run_name, hparams,  X_train=X_train, y_train=y_train):\n",
    "    log_dir= os.path.join(base_dir, run_name)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.mkdir(log_dir)\n",
    "    with tf.summary.create_file_writer(log_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        results = train_test_model(run_name, hparams, log_dir, X_train, y_train)\n",
    "        tf.summary.scalar('loss', results[0], step=1)\n",
    "        tf.summary.scalar('tp', results[1], step=1)\n",
    "        tf.summary.scalar('fp', results[2], step=1)\n",
    "        tf.summary.scalar('tn', results[3], step=1)\n",
    "        tf.summary.scalar('fn', results[4], step=1)\n",
    "        tf.summary.scalar('accuracy', results[5], step=1)\n",
    "        tf.summary.scalar('precision', results[6], step=1)\n",
    "        tf.summary.scalar('recall', results[7], step=1)\n",
    "        tf.summary.scalar('auc', results[8], step=1)\n",
    "        tf.summary.scalar('f1-score', results[9], step=1)\n",
    "                                     \n",
    "def hrun(dataset_name='default', X_train=X_train, y_train=y_train):\n",
    "    session_num=0\n",
    "    for num_units_l1 in HP_NUM_UNITS_L1.domain.values:\n",
    "        for num_units_l2 in HP_NUM_UNITS_L2.domain.values:\n",
    "            for num_units_l3 in HP_NUM_UNITS_L3.domain.values:\n",
    "                for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "                    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "                        for loss in HP_LOSS.domain.values:\n",
    "                            for batch_size in HP_BATCH_SIZE.domain.values:\n",
    "                                hparams = {\n",
    "                                  HP_NUM_UNITS_L1: num_units_l1,\n",
    "                                  HP_NUM_UNITS_L2: num_units_l2,\n",
    "                                  HP_NUM_UNITS_L3: num_units_l3,\n",
    "                                  HP_DROPOUT: dropout_rate,\n",
    "                                  HP_OPTIMIZER: optimizer,\n",
    "                                  HP_LOSS: loss,\n",
    "                                  HP_BATCH_SIZE: batch_size}\n",
    "                                run_name =dataset_name + \"-run-%d\" % session_num\n",
    "                                print('\\n--- ----------Starting trial:',  run_name, '--------------------------')\n",
    "                                print({h.name: hparams[h] for h in hparams})\n",
    "                                run(run_name, hparams,  X_train, y_train)\n",
    "                                session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "generated_datasets_X={}\n",
    "generated_datasets_y={}\n",
    "\n",
    "generated_datasets_X['weighted'] = X_train\n",
    "generated_datasets_y['weighted'] = y_train\n",
    "\n",
    "generated_datasets_X['smote'] = X_train_SMOTE\n",
    "generated_datasets_y['smote'] = y_train_SMOTE\n",
    "\n",
    "generated_datasets_X['smte'] = X_train_smte\n",
    "generated_datasets_y['smte'] = y_train_smte\n",
    "\n",
    "generated_datasets_X['SMOTETomek'] = X_train_SMTomek\n",
    "generated_datasets_y['SMOTETomek'] = y_train_SMTomek\n",
    "\n",
    "generated_datasets_X['ros'] = X_train_ros\n",
    "generated_datasets_y['ros'] = y_train_ros\n",
    "\n",
    "generated_datasets_X['rus'] = X_train_rus\n",
    "generated_datasets_y['rus'] = y_train_rus\n",
    "\n",
    "for key in generated_datasets_X.keys():\n",
    "    print('Running dataset:' + key)\n",
    "    hrun(key,generated_datasets_X.get(key), generated_datasets_y.get(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "p = Pool()\n",
    "p.map(hrun, [(key,generated_datasets_X.get(key), generated_datasets_y.get(key)) for key in generated_datasets_X.keys()])\n",
    "# results = p.map(lambda x: x**2, [1 for key in [1,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "tf.Session(config=tf.ConfigProto(\n",
    "  intra_op_parallelism_threads=NUM_THREADS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "a = os.path.join('logs', 'hparam_tuning', datetime.now().strftime(\"%Y-%m-%d-%H%M\"))\n",
    "b = os.path.join(a, 'b')\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "help(tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "roc_curve(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "row = 300\n",
    "patient = df.drop('stroke', axis = 1).iloc[row]\n",
    "patient = scaler.transform(patient.values.reshape(1,14))\n",
    "patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model.predict_classes(patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "for row in range (0,10):\n",
    "    patient = df.drop('stroke', axis = 1).iloc[row]\n",
    "    patient = scaler.transform(patient.values.reshape(1,13))\n",
    "    prediction = (model.predict_classes(patient))[0][0]\n",
    "    print (\"Predicted:{} Actual:{}\".format(prediction,df.iloc[row]['stroke']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "help(explainer.explain_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "help(lime.lime_tabular.LimeTabularExplainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime import lime_tabular\n",
    "\n",
    "feature_names = df.drop('stroke', axis = 1).columns\n",
    "explainer = lime_tabular.LimeTabularExplainer(X_train_rus,training_labels=[0,1], feature_names=feature_names)\n",
    "# explainer = lime.lime_tabular.LimeTabularExplainer(X_train_rus ,feature_names = df.columns, kernel_width=3)\n",
    "predict_fn = lambda x: model.predict_proba(x).astype(float)\n",
    "exp = explainer.explain_instance(patient[0], predict_fn, labels=['stroke'], top_labels=2, num_features=5)\n",
    "exp.show_in_notebook(show_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model.predict_proba(patient).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "patient[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df.drop('stroke', axis = 1).iloc[row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df.drop('stroke', axis = 1).columns"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}